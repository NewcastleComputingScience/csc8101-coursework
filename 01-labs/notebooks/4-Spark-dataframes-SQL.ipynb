{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "946822ca-4a8f-46f5-a640-f06514482b84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sparl SQL and Dataframes essential docs:\n",
    "\n",
    "- programming guide: https://spark.apache.org/docs/3.2.0/sql-programming-guide.html\n",
    "- live notebook example: https://mybinder.org/v2/gh/apache/spark/5d45a415f3?filepath=python%2Fdocs%2Fsource%2Fgetting_started%2Fquickstart_df.ipynb\n",
    "- example notebooks (git repo): https://github.com/apache/spark/tree/5d45a415f3/examples/src/main/python\n",
    "- Dataframe API: https://spark.apache.org/docs/3.2.0/api/python/reference/pyspark.sql.html#dataframe-apis\n",
    "- DataFrame Quickstart doc: https://spark.apache.org/docs/3.2.0/api/python/getting_started/quickstart_df.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0fd42f82-f15c-4ae3-84ea-16bf4e660997",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark DataFrame and SQL API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca20dd95-56a2-43f2-abcb-adf0402ee49f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- RDDs are (parallelised) collections of objects of some type.\n",
    "- However, RDDs **do not have a schema**. This makes it difficult to perform typical \"relational\" operations.\n",
    "- DataFrames have been introduced to facilitate relational operations, and to bring the abstraction level of Spark data closer to those who are more familiar with python / Pandas / relational tables programming.\n",
    "- DataFrames are very similar to the corresponding concept eg in python / Pandas, however their content can be parallelised in the same way as that of RDD\n",
    "- In fact, under the hood, a DataFrame's native native representation is as an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ec8bb99-78eb-4a87-83f8-695b50f2b4dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We now look at the relationship between DataFrames and RDDs\n",
    "\n",
    "Suppose you are given a dataset containing taxi trips information.\n",
    "Suppose each trip is represented as a record, for instance\n",
    "`1,2019-01-01 00:46:40,2019-01-01 00:53:20,1,1.50,1,N,151,239,1,7,0.5,0.5,1.65,0,0.3,9.95`\n",
    "\n",
    "this dataset can be loaded into a RDD, and Spark will parallelise it under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0984c701-cb75-4576-9189-1e0786f55481",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[1]: [&#39;1,2019-01-01 00:46:40,2019-01-01 00:53:20,1,1.50,1,N,151,239,1,7,0.5,0.5,1.65,0,0.3,9.95,&#39;,\n",
       " &#39;1,2019-01-01 00:59:47,2019-01-01 01:18:59,1,2.60,1,N,239,246,1,14,0.5,0.5,1,0,0.3,16.3,&#39;,\n",
       " &#39;2,2018-12-21 13:48:30,2018-12-21 13:52:40,3,.00,1,N,236,236,1,4.5,0.5,0.5,0,0,0.3,5.8,&#39;,\n",
       " &#39;2,2018-11-28 15:52:25,2018-11-28 15:55:45,5,.00,1,N,193,193,2,3.5,0.5,0.5,0,0,0.3,7.55,&#39;,\n",
       " &#39;2,2018-11-28 15:56:57,2018-11-28 15:58:33,5,.00,2,N,193,193,2,52,0,0.5,0,0,0.3,55.55,&#39;]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[1]: [&#39;1,2019-01-01 00:46:40,2019-01-01 00:53:20,1,1.50,1,N,151,239,1,7,0.5,0.5,1.65,0,0.3,9.95,&#39;,\n &#39;1,2019-01-01 00:59:47,2019-01-01 01:18:59,1,2.60,1,N,239,246,1,14,0.5,0.5,1,0,0.3,16.3,&#39;,\n &#39;2,2018-12-21 13:48:30,2018-12-21 13:52:40,3,.00,1,N,236,236,1,4.5,0.5,0.5,0,0,0.3,5.8,&#39;,\n &#39;2,2018-11-28 15:52:25,2018-11-28 15:55:45,5,.00,1,N,193,193,2,3.5,0.5,0.5,0,0,0.3,7.55,&#39;,\n &#39;2,2018-11-28 15:56:57,2018-11-28 15:58:33,5,.00,2,N,193,193,2,52,0,0.5,0,0,0.3,55.55,&#39;]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tripsRDD = sc.textFile('/FileStore/tables/taxi/yellow_tripdata_2019_01_HEAD_1000_noheader.txt')\n",
    "\n",
    "tripsRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4db80db8-ebca-481d-8bc2-095d7a1de799",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This RDD can be processed efficiently, however it does not carry any information about the meaning of the elements.\n",
    "\n",
    "Suppose we know that the individual fields have the interpretation provided in the [NYC Taxi user guide](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page): \n",
    "\n",
    "`[VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge]`\n",
    "\n",
    "**Exercise:** using `tripsRDD`, calculate the cost per mile `total_amount/trip_distance` for each trip, and then return the average of those values. Use `map()` and `reduce()` to achieve this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9e1d8ed7-edf1-4654-9c85-a4c612888d2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Suppose you are given a CSV file with the same data, complete with header:\n",
    "\n",
    "`VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount,congestion_surcharge`\n",
    "`1,2019-01-01 00:46:40,2019-01-01 00:53:20,1,1.50,1,N,151,239,1,7,0.5,0.5,1.65,0,0.3,9.95,`\n",
    "`1,2019-01-01 00:59:47,2019-01-01 01:18:59,1,2.60,1,N,239,246,1,14,0.5,0.5,1,0,0.3,16.3,`\n",
    "\n",
    "here we have previously saved this as parquet file: `taxi/yellow_tripdata_2019_01_HEAD_1000.parquet`\n",
    "\n",
    "We can load this file directly into a DataFrame, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f41976a7-2313-4d22-a516-96f090428da0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tripsDF = spark.read.parquet('/FileStore/tables/taxi/yellow_tripdata_2019_01_HEAD_1000.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "73416823-8c39-496f-8705-fab3af5db669",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "this DataFrame carries column names and types, so it is effectively a relational table in the classic database sense. It is also consistent with Pandas DataFrames\n",
    "\n",
    "if we try to replicate the exercise above, we find that the RDD operators `map()` and `reduce()` (and many others) are not available. \n",
    "\n",
    "However, the [DataFrame API](https://spark.apache.org/docs/3.2.0/api/python/reference/pyspark.sql.html#dataframe-apis) offers higher level operators that let us accomplish the same task easily.\n",
    "\n",
    "The following 1-line code illustrates some of the key concepts in working with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ad04ce3-a958-47e4-ba88-88e20bc8edf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[3]: [Row(avg(unit_cost)=7.636152863587667)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[3]: [Row(avg(unit_cost)=7.636152863587667)]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## tripsDF['total_amount']/tripsDF['trip_distance']  operates on each row in the columns, and returns a spark.sql.Column object\n",
    "\n",
    "## tripsDF.withColumn('unit_cost', <col>) adds a column defined by <col> named 'unit_cost' to the DataFrame\n",
    "\n",
    "## <column>.groupby() constructs a group out of the entire <column>, returning a GroupedData  object which contains a single group with every value in the column\n",
    "\n",
    "## <GroupedData>.mean(<col>) applies the built-in <mean> function to the values in <col> (for each group, in general)\n",
    "\n",
    "## collect() triggers the entire computation and returns a value\n",
    "\n",
    "\n",
    "tripsDF.withColumn('unit_cost', tripsDF['total_amount']/tripsDF['trip_distance']).groupBy().mean('unit_cost').collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21c5a055-d461-4fcf-8613-ceafab266277",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "you will notice that the result is expressed using a `Row` object. Each DataFrame has as underlying representation as RDD, where the type of the RDD is `spark.sql.Row`. \n",
    "This representation is accessed simply using\n",
    "\n",
    "`df.rdd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a2c104e1-605e-41aa-89d1-28f26e97f1e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[5]: MapPartitionsRDD[16] at javaToPython at NativeMethodAccessorImpl.java:0</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[5]: MapPartitionsRDD[16] at javaToPython at NativeMethodAccessorImpl.java:0</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tripsDF.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4847c9b3-36b7-4f64-971c-4c2be39b5dd1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "if you `collect()` or `take(n)` the content of the RDD, you can see the `Row` structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0c1d6bd9-c39e-4176-a75a-0cdbd1f38557",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[7]: [Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2019, 1, 1, 0, 46, 40), tpep_dropoff_datetime=datetime.datetime(2019, 1, 1, 0, 53, 20), passenger_count=1, trip_distance=1.5, RatecodeID=1, store_and_fwd_flag=&#39;N&#39;, PULocationID=151, DOLocationID=239, payment_type=1, fare_amount=7.0, extra=0.5, mta_tax=0.5, tip_amount=1.65, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=9.95, congestion_surcharge=None),\n",
       " Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2019, 1, 1, 0, 59, 47), tpep_dropoff_datetime=datetime.datetime(2019, 1, 1, 1, 18, 59), passenger_count=1, trip_distance=2.6, RatecodeID=1, store_and_fwd_flag=&#39;N&#39;, PULocationID=239, DOLocationID=246, payment_type=1, fare_amount=14.0, extra=0.5, mta_tax=0.5, tip_amount=1.0, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=16.3, congestion_surcharge=None)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[7]: [Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2019, 1, 1, 0, 46, 40), tpep_dropoff_datetime=datetime.datetime(2019, 1, 1, 0, 53, 20), passenger_count=1, trip_distance=1.5, RatecodeID=1, store_and_fwd_flag=&#39;N&#39;, PULocationID=151, DOLocationID=239, payment_type=1, fare_amount=7.0, extra=0.5, mta_tax=0.5, tip_amount=1.65, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=9.95, congestion_surcharge=None),\n Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2019, 1, 1, 0, 59, 47), tpep_dropoff_datetime=datetime.datetime(2019, 1, 1, 1, 18, 59), passenger_count=1, trip_distance=2.6, RatecodeID=1, store_and_fwd_flag=&#39;N&#39;, PULocationID=239, DOLocationID=246, payment_type=1, fare_amount=14.0, extra=0.5, mta_tax=0.5, tip_amount=1.0, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=16.3, congestion_surcharge=None)]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tripsDF.rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1c8080ec-c9b9-4776-8076-0b26ac40699b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "thus if you wanted to use `map()` for example, you could operate on `df.rdd`, with the caveat that you need to be aware that the data is now incapsulated into `Row` objects\n",
    "\n",
    "individual fields in a Row can be accessed using `row.key` or `row[key]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "23abe919-bb42-44a4-b169-3d65f7819f05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark and Pandas dataframes\n",
    "\n",
    "in the most recent version of Spark (3.2), you can also transform a Spark dataframe into a Pandas dataframe, and viceversa.\n",
    "\n",
    "Spark provides an  API to interface with Pandas:\n",
    "`import pyspark.pandas as ps` gives you access to Pandas DataFrames that are virtually interchangeable.\n",
    "\n",
    "See [examples here](https://spark.apache.org/docs/3.2.0/api/python/user_guide/pandas_on_spark/pandas_pyspark.html) \n",
    "and [here](https://sparkbyexamples.com/pyspark/convert-pyspark-dataframe-to-pandas/)\n",
    "\n",
    "Beware however, that if you simply use the nativa Pandas API:\n",
    "`import pandas as pd` then you lose the benefit of distributed processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6e2ef10-6f99-46ac-ad5b-2ff8f60fcbcc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark SQL\n",
    "\n",
    "since DataFrames are essentially relational tables, it makes sense to use SQL to operate on them.\n",
    "\n",
    "This is accomplished using the spark.sql API, see here: https://spark.apache.org/docs/3.2.0/sql-getting-started.html#running-sql-queries-programmatically\n",
    "\n",
    "the only requirement is that the DataFrame be mapped to a SQL table. This is done simply by:\n",
    "\n",
    "`df.createOrReplaceTempView(<table name>)`\n",
    "\n",
    "then you can use `<table name>` as a regular table name in a FROM clause in an SQL expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bc101960-01a2-43d0-9998-5c76db9a9a02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----------------+\n",
       "    avg_unit_cost|\n",
       "+-----------------+\n",
       "7.636152863587667|\n",
       "+-----------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+-----------------+\n|    avg_unit_cost|\n+-----------------+\n|7.636152863587667|\n+-----------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## compute the average unit cost for a trip:\n",
    "\n",
    "tripsDF.createOrReplaceTempView('tripsTable')\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT avg(total_amount/trip_distance) as avg_unit_cost FROM tripsTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "373bbafa-71ac-48a7-abae-1263ddba36a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Exercise** \n",
    "\n",
    "calculate the average trip_distance for trips longer than 3 miles, for each of the possible payment types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ca16065-3503-484e-9a39-90d84cc9d28b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+------------+------------------+\n",
       "payment_type|avg(trip_distance)|\n",
       "+------------+------------------+\n",
       "           1| 5.944899999999999|\n",
       "           3|              15.1|\n",
       "           2| 6.695229357798162|\n",
       "+------------+------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+------------+------------------+\n|payment_type|avg(trip_distance)|\n+------------+------------------+\n|           1| 5.944899999999999|\n|           3|              15.1|\n|           2| 6.695229357798162|\n+------------+------------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"SELECT payment_type, avg(trip_distance) from tripsTable where trip_distance > 3 group by payment_type\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "40120137-086c-408a-84ad-a469b76966a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark-dataframes",
   "notebookOrigID": 214449445958423,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
